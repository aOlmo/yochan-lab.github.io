<!DOCTYPE html>

<html lang="en">

    <head>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <link rel="icon" href="favicon.ico">

        <title>Yochan @ FAIM 2018</title>

        <!-- Bootstrap core CSS -->
        <link href="../ASSETS/bootstrap-4.0.0/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom styles for this template -->
        <link href="./files/css/custom.css" rel="stylesheet">

        <!-- jQuery -->
        <script src="../ASSETS/jquery/jquery-3.2.1.min.js"></script>

        <!-- Bootstrap core JavaScript
        ================================================== -->
        <script src="../ASSETS/bootstrap-4.0.0/js/popper.min.js"></script>
        <script src="../ASSETS/bootstrap-4.0.0/js/bootstrap.min.js"></script>

        <!-- Custom JavaScript -->
        <script src="./files/js/custom.js"></script>

    </head>

    <body class="bg-light" id="body" data-spy="scroll" data-target="#body" data-offset="100">

        <nav class="navbar navbar-expand-md fixed-top navbar-dark bg-dark">
            <a class="navbar-brand spy-enabled" href="#body"><strong>Yochan @ FAIM 2018</strong></a>
        </nav>

        <div class="nav-scroller bg-white box-shadow">
            <nav class="nav nav-underline">
                <a class="nav-link bold" href="http://rakaposhi.eas.asu.edu/yochan/">Home</a>
                <a class="nav-link bold" href="https://robotics.asu.edu/">ASU</a>
                <a class="nav-link bold" href="http://www.ae-robots.com/">&AElig;Robotics</a>
                <a class="nav-link nav-link-no-link d-none d-sm-block">Last Updated <span class="badge badge-pill bg-light align-text-bottom">2018-07-13</span></a>
            </nav>
        </div>

        <main role="main" class="container">

        <div class="d-flex align-items-center p-3 my-3 text-white-50 bg-maroon rounded box-shadow">
            <img class="mr-3" src="./files/images/icon.png" alt="" width="48" height="48" style="border-radius: 3pt;">
            <div class="lh-100">
                <h6 class="mb-0 text-white lh-100">Yochan Lab | Arizona State University</h6>
                <small>PI: Subbarao Kambhampati | <a class="nav-link-yellow text-white-50" href="mailto:rao@asu.edu">rao@asu.edu</a> | <a class="nav-link-yellow text-white-50" href="https://twitter.com/rao2z">@rao2z</a> | <a class="nav-link-yellow text-white-50" href="http://rakaposhi.eas.asu.edu/">rakaposhi.eas.asu.edu</a> </small>
            </div>
        </div>

<div class="my-3 p-3 bg-white rounded box-shadow" id="IJCAI-2018">

<h6 class="border-bottom border-gray pb-2 mb-0">IJCAI 2018</h6>

<div id="IJCAI 2018">

<div>

<div class="media text-muted pt-3 demo-link" data-toggle="collapse" href="#collapse-1-IJCAI-2018" aria-expanded="false" aria-controls="collapse-1-IJCAI-2018">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Human-Aware Planning: A Tale of Three Models</strong>
<a class="paper-link text-right" https://yochan-lab.github.io/papers/files/papers/three-models.pdf>Paper</a>
</div>
<span class="d-block">Tathagata Chakraborti, Sarath Sreedharan and Subbarao Kambhampati</span>
<br>
<span class="d-block">XAI Workshop Friday 7/13 Posters 17:30-19:00</span>
</div>

</div>

<div id="collapse-1-IJCAI-2018" class="collapse" role="tabpanel" data-parent="#accordion-1-IJCAI-2018">
<div class="card-body">
<p style="color:black;">
Human-aware planning requires an agent to be aware of the mental model of the humans, in addition to their physical or capability model. This not only allows an agent to envisage the desired roles of the human in a joint plan but also anticipate how its plan will be perceived by the latter. The human mental model becomes especially useful in the context of an explainable planning (XAIP) agent since an explanatory process cannot be a soliloquy, i.e. it must incorporate the human’s beliefs and expectations of the planner. In this paper, we survey our recent efforts in this direction.
</p>
</div>
</div>

</div>
<div>

<div class="media text-muted pt-3 demo-link" data-toggle="collapse" href="#collapse-2-IJCAI-2018" aria-expanded="false" aria-controls="collapse-2-IJCAI-2018">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Recognizing Plans by Learning Embeddings from Observed Action Distributions </strong>
<a class="paper-link text-right" https://arxiv.org/abs/1712.01949>Paper</a>
</div>
<span class="d-block">Yantian Zha, Yikang Li, Sriram Gopalakrishnan, Baoxin Li, Subbarao Kambhampati </span>
<br>
<span class="d-block">ICML / IJCAI / AAMAS 2018 Workshop on Planning and Learning (PAL-18) Sunday 7/15/18 15:00</span>
</div>

</div>

<div id="collapse-2-IJCAI-2018" class="collapse" role="tabpanel" data-parent="#accordion-2-IJCAI-2018">
<div class="card-body">
<p style="color:black;">
Recent advances in visual activity recognition have raised the possibility of applications such as automated video surveillance. Effective approaches for such problems however require the ability to recognize the plans of the agents from video information. Although traditional plan recognition algorithms depend on access to sophisticated domain models, one recent promising direction involves learning shallow models directly from the observed activity sequences, and using them to recognize/predict plans. One limitation of such approaches is that they expect observed action sequences as training data. In many cases involving vision or sensing from raw data, there is considerably uncertainty about the specific action at any given time point. The most we can expect in such cases is probabilistic information about the action at that point. The training data will then be sequences of such observed action distributions. In this paper, we focus on doing effective plan recognition with such uncertain observations. Our contribution is a novel extension of word vector embedding techniques to directly handle such observation distributions as input. This involves computing embeddings by minimizing the distance between distributions (measured as KL-divergence). We will show that our approach has superior performance when the perception error rate (PER) is higher, and competitive performance when the PER is lower. We will also explore the possibility of using importance sampling techniques to handle observed action distributions with traditional word vector embeddings. We will show that although such approaches can give good recognition accuracy, they take significantly longer training time and their performance will degrade significantly at higher perception error rate.
</p>
</div>
</div>

</div>
<div>

<div class="media text-muted pt-3 demo-link" data-toggle="collapse" href="#collapse-3-IJCAI-2018" aria-expanded="false" aria-controls="collapse-3-IJCAI-2018">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Extracting Action Sequences from Texts Based on Deep Reinforcement Learning</strong>
<a class="paper-link text-right" https://www.ijcai.org/proceedings/2018/0565.pdf>Paper</a>
</div>
<span class="d-block">Wenfeng Feng, Hankz Hankui Zhuo, Subbarao Kambhampati</span>
<br>
<span class="d-block">IJCAI Main Track Planning and Learning Session (K2) (Monday 7/16/18 14:55 - 16:10)</span>
</div>

</div>

<div id="collapse-3-IJCAI-2018" class="collapse" role="tabpanel" data-parent="#accordion-3-IJCAI-2018">
<div class="card-body">
<p style="color:black;">
Extracting action sequences from texts is challenging, as it requires commonsense inferences based on world knowledge. Although there has been work on extracting action scripts, instructions, navigation actions, etc., they require either the set of candidate actions be provided in advance, or action descriptions are restricted to a specific form, e.g., description templates. In this paper we aim to extract action sequences from texts in free natural language, i.e., without any restricted templates, provided the set of actions is unknown. We propose to extract action sequences from texts based on the deep reinforcement learning framework. Specifically, we view “selecting” or “eliminating” words from texts as “actions”, and texts associated with actions as “states”. We build Q-networks to learn policies of extracting actions and extract plans from the labeled texts. We demonstrate the effectiveness of our approach on several datasets with comparison to state-of-the-art approaches
</p>
</div>
</div>

</div>
<div>

<div class="media text-muted pt-3 demo-link" data-toggle="collapse" href="#collapse-4-IJCAI-2018" aria-expanded="false" aria-controls="collapse-4-IJCAI-2018">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Visualizations for an Explainable Planning Agent</strong>
<a class="paper-link text-right" https://arxiv.org/pdf/1709.04517.pdf>Paper</a>
</div>
<span class="d-block">Tathagata Chakraborti, Kshitij P. Fadnis, Kartik Talamadupula, Mishal Dholakia, Biplav Srivastava, Jeffrey O. Kephart, Rachel K. E. Bellamy</span>
<br>
<span class="d-block">IJCAI Demo Track Tuesday 7/17/18 11:20 - 12:45 (Oral) and afternoon (demo presentation)</span>
</div>

</div>

<div id="collapse-4-IJCAI-2018" class="collapse" role="tabpanel" data-parent="#accordion-4-IJCAI-2018">
<div class="card-body">
<p style="color:black;">
In this paper, we report on the visualization capabilities of an Explainable AI Planning (XAIP) agent that can support human in the loop decision making. Imposing transparency and explainability requirements on such agents is especially important in order to establish trust and common ground with the end-to-end automated planning system. Visualizing the agent’s internal decision making processes is a crucial step towards achieving this. This may include externalizing the “brain” of the agent – starting from its sensory inputs, to progressively higher order decisions made by it in order to drive its planning components. We also show how the planner can bootstrap on the latest techniques in explainable planning to cast plan visualization as a plan explanation problem, and thus provide concise model based visualization of its plans. We demonstrate these functionalities in the context of the automated planning components of a smart assistant in an instrumented meeting space.
</p>
</div>
</div>

</div>
<div>

<div class="media text-muted pt-3 demo-link" data-toggle="collapse" href="#collapse-5-IJCAI-2018" aria-expanded="false" aria-controls="collapse-5-IJCAI-2018">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Hierarchical Expertise Level Modeling for User Specific Contrastive Explanations</strong>
<a class="paper-link text-right" https://www.ijcai.org/proceedings/2018/0671.pdf>Paper</a>
</div>
<span class="d-block">Sarath Sreedharan, Siddharth Srivastava, Subbarao Kambhampati</span>
<br>
<span class="d-block">IJCAI Main Track Planning and Scheduling Session (K2) (Wednesday 7/18/18 16:40 - 18:20)</span>
</div>

</div>

<div id="collapse-5-IJCAI-2018" class="collapse" role="tabpanel" data-parent="#accordion-5-IJCAI-2018">
<div class="card-body">
<p style="color:black;">
There is a growing interest within the AI research community in developing autonomous systems capable of explaining their behavior to users. However, the problem of computing explanations for users of different levels of expertise has received little research attention. We propose an approach for addressing this problem by representing the user’s understanding of the task as an abstraction of the domain model that the planner uses. We present algorithms for generating minimal explanations in cases where this abstract human model is not known. We reduce the problem of generating an explanation to a search over the space of abstract models and show that while the complete problem is NP-hard, a greedy algorithm can provide good approximations of the optimal solution. We also empirically show that our approach can efficiently compute explanations for a variety of problems.
</p>
</div>
</div>

</div>


</div>

</div>
<div class="my-3 p-3 bg-white rounded box-shadow" id="AAMAS-2018">

<h6 class="border-bottom border-gray pb-2 mb-0">AAMAS 2018</h6>

<div id="AAMAS 2018">

<div>

<div class="media text-muted pt-3 demo-link" data-toggle="collapse" href="#collapse-1-AAMAS-2018" aria-expanded="false" aria-controls="collapse-1-AAMAS-2018">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Recognizing Plans by Learning Embeddings from Observed Action Distributions </strong>
<a class="paper-link text-right" https://arxiv.org/abs/1712.01949>Paper</a>
</div>
<span class="d-block">Yantian Zha, Yikang Li, Sriram Gopalakrishnan, Baoxin Li, Subbarao Kambhampati </span>
<br>
<span class="d-block">AAMAS Main Track Posters Wednesday 7/11 15:30-17:00</span>
</div>

</div>

<div id="collapse-1-AAMAS-2018" class="collapse" role="tabpanel" data-parent="#accordion-1-AAMAS-2018">
<div class="card-body">
<p style="color:black;">
Recent advances in visual activity recognition have raised the possibility of applications such as automated video surveillance. Effective approaches for such problems however require the ability to recognize the plans of the agents from video information. Although traditional plan recognition algorithms depend on access to sophisticated domain models, one recent promising direction involves learning shallow models directly from the observed activity sequences, and using them to recognize/predict plans. One limitation of such approaches is that they expect observed action sequences as training data. In many cases involving vision or sensing from raw data, there is considerably uncertainty about the specific action at any given time point. The most we can expect in such cases is probabilistic information about the action at that point. The training data will then be sequences of such observed action distributions. In this paper, we focus on doing effective plan recognition with such uncertain observations. Our contribution is a novel extension of word vector embedding techniques to directly handle such observation distributions as input. This involves computing embeddings by minimizing the distance between distributions (measured as KL-divergence). We will show that our approach has superior performance when the perception error rate (PER) is higher, and competitive performance when the PER is lower. We will also explore the possibility of using importance sampling techniques to handle observed action distributions with traditional word vector embeddings. We will show that although such approaches can give good recognition accuracy, they take significantly longer training time and their performance will degrade significantly at higher perception error rate.
</p>
</div>
</div>

</div>
<div>

<div class="media text-muted pt-3 demo-link" data-toggle="collapse" href="#collapse-2-AAMAS-2018" aria-expanded="false" aria-controls="collapse-2-AAMAS-2018">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Balancing Explanations and Explicability in Human-Aware Planning</strong>
<a class="paper-link text-right" https://arxiv.org/pdf/1708.00543.pdf>Paper</a>
</div>
<span class="d-block">Tathagata Chakraborti, Sarath Sreedharan and Subbarao Kambhampati</span>
<br>
<span class="d-block">AAMAS Robotics Track Posters Friday 7/13 15:30-17:00</span>
</div>

</div>

<div id="collapse-2-AAMAS-2018" class="collapse" role="tabpanel" data-parent="#accordion-2-AAMAS-2018">
<div class="card-body">
<p style="color:black;">
Human aware planning requires an agent to be aware of the intentions, capabilities and mental model of the human in the loop during its decision process. This can involve generating plans that are explicable to a human observer as well as the ability to provide explanations when such plans cannot be generated. In this paper, we bring these two concepts together and show how an agent can account for both these needs and achieve a trade-off during the plan generation process itself by means of a model-space search method MEGA. This in effect provides a comprehensive perspective of what it means for a decision-making agent to be “human-aware” by bringing together existing principles of planning under the umbrella of a single plan generation process. We situate our discussion in the context of recent work on explicable planning and explanation generation, and illustrate these concepts in modified versions of two well-known planning domains, as well as in a demonstration of a robot involved in a typical search and reconnaissance task with an external supervisor. Human factor studies in the latter highlight the usefulness of the proposed approaches.
</p>
</div>
</div>

</div>


</div>

</div>


        <div class="d-block" style="margin-top: 30px; margin-bottom: 30px;">
            <img src="files/images/robots.png" width="100%">
        </div>

        </main>


        <div class="footer-container">

            <div class="footer-bottom-left" style="width:100%;">

                <hr>

                <footer class="text-center" style="color:gray;">
                    <p>&copy; Yochan 2018</p>
                </footer>

            </div>

        </div>

    </body>

</html>