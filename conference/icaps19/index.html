<!DOCTYPE html>

<html lang="en">

    <head>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

        <link rel="icon" href="../../favicon.ico">

        <title>Yochan @ ICAPS 2019</title>

        <!-- Bootstrap core CSS -->
        <link href="../../ASSETS/bootstrap-4.0.0/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom styles for this template -->
        <link href="./../files/css/custom.css" rel="stylesheet">

        <!-- jQuery -->
        <script src="../../ASSETS/jquery/jquery-3.2.1.min.js"></script>

        <!-- Bootstrap core JavaScript
        ================================================== -->
        <script src="../../ASSETS/bootstrap-4.0.0/js/popper.min.js"></script>
        <script src="../../ASSETS/bootstrap-4.0.0/js/bootstrap.min.js"></script>

        <!-- Custom JavaScript -->
        <script src="./../files/js/custom.js"></script>

    </head>

    <body class="bg-light" id="body" data-spy="scroll" data-target="#body" data-offset="100">

        <nav class="navbar navbar-expand-md fixed-top navbar-dark bg-dark">
            <li class="navbar-brand spy-enabled text-light"><strong>Yochan @ ICAPS 2019</strong></li>
        </nav>

        <div class="nav-scroller bg-white box-shadow">
            <nav class="nav nav-underline">
                <a class="nav-link bold" href="http://rakaposhi.eas.asu.edu/yochan/">Yochan</a>
                <a class="nav-link bold" href="https://robotics.asu.edu/">ASU Robotics</a>
                <a class="nav-link nav-link-no-link d-none d-sm-block">Last Updated <span class="badge badge-pill bg-light align-text-bottom">2019-07-06</span></a>
            </nav>
        </div>

        <main role="main" class="container">

        <div class="d-flex align-items-center p-3 my-3 text-white-50 bg-maroon rounded box-shadow">
            <img class="mr-3" src="./../files/images/icon.png" width="48" height="48" style="border-radius: 3pt;">
            <div class="lh-100">
                <h6 class="mb-0 text-white lh-100">Yochan Lab | Arizona State University</h6>
                <small>PI: Subbarao Kambhampati | <a class="nav-link-yellow text-white-50" href="mailto:rao@asu.edu">rao@asu.edu</a> | <a class="nav-link-yellow text-white-50" href="https://twitter.com/rao2z">@rao2z</a> | <a class="nav-link-yellow text-white-50" href="http://rakaposhi.eas.asu.edu/">rakaposhi.eas.asu.edu</a> </small>
            </div>
        </div>

<div class="my-3 p-3 bg-white rounded box-shadow" id="ICAPS-2019">

<h6 class="border-bottom border-gray pb-2 mb-0">ICAPS 2019</h6>

<div id="ICAPS-2019">

<div id="accordion-ICAPS-2019" role="tablist">

<div>

<div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-1-ICAPS-2019" aria-expanded="false" aria-controls="collapse-1-ICAPS-2019">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Explainable Planning for Human-Robot Collaboration</strong>
</div>
<span class="d-block">Subbarao Kambhampati</span>
<br>
<span class="d-block"><em>Jul 11 Thursday <b>2pm</b> (Room 310) in Workshop on Planning and Robotics (PlanRob)</em></span>
<a class="paper-link text-right badge badge-success" href="https://icaps19.icaps-conference.org/workshops/PlanRob" target="_blank">Link</a>
<a class="paper-link text-right badge badge-warning" href="https://icaps19.icaps-conference.org/workshops/PlanRob" target="_blank">Invited Talk</a>

</div>

</div>

<div id="collapse-1-ICAPS-2019" class="collapse" role="tabpanel" data-parent="#accordion-ICAPS-2019">
<div class="card-body">
<p style="color:black;">
    As robots enter our homes and work places, there is a greater need for them to work synergistically with humans. This requires the robots to exhibit behavior that is explainable to humans. Synthesizing such
behavior requires the robots  to reason not only with their own models of the task at hand, but also about the mental models of the human collaborators. Using several case-studies from our ongoing research, I will discuss how such multi-model planning forms the basis for explainable behavior for human-robot collaboration and teaming.
</p>
</div>
<hr>
</div>

</div>
<div>

<div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-2-ICAPS-2019" aria-expanded="false" aria-controls="collapse-2-ICAPS-2019">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Design for Interpretability</strong>
</div>
<span class="d-block">Anagha Kulkarni, Sarath Sreedharan, Sarah Keren, Tathagata Chakraborti, David E. Smith, Subbarao Kambhampati</span>
<br>
<span class="d-block"><em>Jul 12 Friday <b>9:45am</b> (Room 310) in Workshop on Explainable Planning (XAIP) + all day poster session</em></span>
<a class="paper-link text-right badge badge-success" href="https://openreview.net/forum?id=rkxg4a3m9N" target="_blank">Link</a>

</div>

</div>

<div id="collapse-2-ICAPS-2019" class="collapse" role="tabpanel" data-parent="#accordion-ICAPS-2019">
<div class="card-body">
<p style="color:black;">
    The interpretability of an AI agent's behavior is of utmost importance for effective human-AI interaction. To this end, there has been increasing interest in characterizing and generating interpretable behavior of the agent. An alternative approach to guarantee that the agent generates interpretable behavior would be to design the agent's environment such that uninterpretable behaviors are either prohibitively expensive or unavailable to the agent. To date, there has been work under the umbrella of goal or plan recognition design exploring this notion of environment redesign in some specific instances of interpretable of behavior. In this position paper, we scope the landscape of interpretable behavior and environment redesign in all its different flavors. Specifically, we focus on three specific types of interpretable behaviors -- explicability, legibility, and predictability -- and present a general framework for the problem of environment design that can be instantiated to achieve each of the three interpretable behaviors. We also discuss how specific instantiations of this framework correspond to prior works on environment design and identify exciting opportunities for future work.
</p>
</div>
<hr>
</div>

</div>
<div>

<div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-3-ICAPS-2019" aria-expanded="false" aria-controls="collapse-3-ICAPS-2019">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Feature-directed Active Learning for Learning User Preferences</strong>
</div>
<span class="d-block">Sriram Gopalakrishnan, Utkarsh Soni, Subbarao Kambhampati</span>
<br>
<span class="d-block"><em>Jul 12 Friday <b>11:10am</b> (Room 310) in Workshop on Explainable Planning (XAIP) + all day poster session</em></span>
<a class="paper-link text-right badge badge-success" href="https://openreview.net/forum?id=BkGeETnQcE" target="_blank">Link</a>

</div>

</div>

<div id="collapse-3-ICAPS-2019" class="collapse" role="tabpanel" data-parent="#accordion-ICAPS-2019">
<div class="card-body">
<p style="color:black;">
    Learning preferences of users over plan traces can be a challenging task given a large number of features and limited queries that we can ask a single user. Additionally, the preference function itself can be quite convoluted and non-linear. Our approach uses feature-directed active learning to gather the necessary information about plan trace preferences. This data is used to train a simple feedforward neural network to learn preferences over the sequential data. We evaluate the impact of active learning on the number of traces that are needed to train a model that is accurate and interpretable. This evaluation is done by comparing the aforementioned feedforward network to a more complex neural network model that uses LSTMs and is trained with a larger dataset without active learning.
</p>
</div>
<hr>
</div>

</div>
<div>

<div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-4-ICAPS-2019" aria-expanded="false" aria-controls="collapse-4-ICAPS-2019">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Model-Free Model Reconciliation</strong>
</div>
<span class="d-block">Sarath Sreedharan, Alberto Olmo, Aditya Prasad Mishra, Subbarao Kambhampati</span>
<br>
<span class="d-block"><em>Jul 12 Friday <b>11:30am</b> (Room 310) in Workshop on Explainable Planning (XAIP) + all day poster session</em></span>
<a class="paper-link text-right badge badge-success" href="https://openreview.net/forum?id=Byex4T2XcV" target="_blank">Link</a>

</div>

</div>

<div id="collapse-4-ICAPS-2019" class="collapse" role="tabpanel" data-parent="#accordion-ICAPS-2019">
<div class="card-body">
<p style="color:black;">
    Designing agents capable of explaining complex sequential decisions remain a significant open problem in automated decision-making. Recently, there has been a lot of interest in developing approaches for generating such explanations for various decision-making paradigms. One such approach has been the idea of explanation as model-reconciliation. The framework hypothesizes that one of the common reasons for the user's confusion could be the mismatch between the user's model of the task and the one used by the system to generate the decisions. While this is a general framework, most works that have been explicitly built on this explanatory philosophy have focused on settings where the model of user's knowledge is available in a declarative form. Our goal in this paper is to adapt the model reconciliation approach to the cases where such user models are no longer explicitly provided. We present a simple and easy to learn labeling model that can help an explainer decide what information could help achieve model reconciliation between the user and the agent.
</p>
</div>
<hr>
</div>

</div>
<div>

<div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-5-ICAPS-2019" aria-expanded="false" aria-controls="collapse-5-ICAPS-2019">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">A General Framework for Synthesizing and Executing Self-Explaining Plans for Human-AI Interaction</strong>
</div>
<span class="d-block">Sarath Sreedharan, Tathagata Chakraborti, Christian Muise, Subbarao Kambhampati</span>
<br>
<span class="d-block"><em>Jul 12 Friday <b>11:50am</b> (Room 310) in Workshop on Explainable Planning (XAIP) + all day poster session</em></span>
<a class="paper-link text-right badge badge-success" href="https://openreview.net/forum?id=H1gyE6hm5V" target="_blank">Link</a>

</div>

</div>

<div id="collapse-5-ICAPS-2019" class="collapse" role="tabpanel" data-parent="#accordion-ICAPS-2019">
<div class="card-body">
<p style="color:black;">
    In this work, we present a general formulation for decision making in human-in-the-loop planning problems where the human's expectations about an autonomous agent may differ from the agent's own model. We show how our formulation for such multi-model planning problems allows us to capture existing approaches to this problem and also be used to generate novel explanatory behaviors. Our formulation also reveals a deep connection between multi-model planning and epistemic planning and we show how we can leverage classical planning compilations designed for epistemic planning for solving multi-model planning problems.  We empirically show how this new compilation provides a computational advantage over previous approaches that separate reasoning about model reconciliation and identifying the agent's plan.
</p>
</div>
<hr>
</div>

</div>
<div>

<div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-6-ICAPS-2019" aria-expanded="false" aria-controls="collapse-6-ICAPS-2019">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Why Couldn't You do that? Explaining Unsolvability of Classical Planning Problems in the Presence of Plan Advice</strong>
</div>
<span class="d-block">Sarath Sreedharan, Siddharth Srivastava, David Smith, Subbarao Kambhampati</span>
<br>
<span class="d-block"><em>Jul 12 Friday <b>2:40pm</b> (Room 310) in Workshop on Explainable Planning (XAIP) + all day poster session</em></span>
<a class="paper-link text-right badge badge-success" href="https://openreview.net/forum?id=HkeCXpnQ94" target="_blank">Link</a>

</div>

</div>

<div id="collapse-6-ICAPS-2019" class="collapse" role="tabpanel" data-parent="#accordion-ICAPS-2019">
<div class="card-body">
<p style="color:black;">
    Explainable planning is widely accepted as a prerequisite for autonomous agents to successfully work with humans. While there has been a lot of research on generating explanations of solutions to planning problems, explaining the absence of solutions remains an open and under-studied problem, even though such situations can be the hardest to understand or debug. In this paper, we show that hierarchical abstractions can be used to efficiently generate reasons for unsolvability of planning problems. In contrast to related work on computing certificates of unsolvability, we show that these methods can generate compact, human-understandable reasons for unsolvability. Empirical analysis and user studies show the validity of our methods  as well as their computational efficacy on a number of benchmark planning domains.
</p>
</div>
<hr>
</div>

</div>
<div>

<div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-7-ICAPS-2019" aria-expanded="false" aria-controls="collapse-7-ICAPS-2019">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">(How) Can AI Bots Lie?</strong>
</div>
<span class="d-block">Tathagata Chakraborti, Subbarao Kambhampati</span>
<br>
<span class="d-block"><em>Jul 12 Friday <b>4:10pm</b> (Room 310) in Workshop on Explainable Planning (XAIP) + all day poster session</em></span>
<a class="paper-link text-right badge badge-success" href="https://openreview.net/forum?id=HkxbEp2QqE" target="_blank">Link</a>

</div>

</div>

<div id="collapse-7-ICAPS-2019" class="collapse" role="tabpanel" data-parent="#accordion-ICAPS-2019">
<div class="card-body">
<p style="color:black;">
    Recent work on explanation generation for decision-making problems has viewed the explanation process as one of model reconciliation where an AI agent brings the human mental model (of its capabilities, beliefs, and goals) to the same page with regards to a task at hand. This formulation succinctly captures many possible types of explanations, as well as explicitly addresses the various properties -- e.g. the social aspects, contrastiveness, and selectiveness -- of explanations studied in social sciences among human-human interactions. However, it turns out that the same process can be hijacked into producing "alternative explanations" -- i.e. explanations that are not true but still satisfy all the properties of a proper explanation. In previous work, we have looked at how such explanations may be perceived by the human in the loop and alluded to one possible way of generating them. In this paper, we go into more details of this curious feature of the model reconciliation process and discuss similar implications to the overall notion of explainable decision-making.
</p>
</div>
<hr>
</div>

</div>
<div>

<div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-8-ICAPS-2019" aria-expanded="false" aria-controls="collapse-8-ICAPS-2019">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Foundations of Human-Aware Planning -- A Tale of Three Models</strong>
</div>
<span class="d-block">Tathagata Chakraborti</span>
<br>
<span class="d-block"><em>Jul 14 Sunday <b>2pm</b> in Sessions 8a: Recognition, Goal and Model Reasoning</em></span>
<a class="paper-link text-right badge badge-success" href="http://tchakra2.com/thesis/" target="_blank">Link</a>
<a class="paper-link text-right badge badge-warning" href="https://icaps19.icaps-conference.org/awards" target="_blank">ICAPS Best Disseratation  Honorable Mention</a>

</div>

</div>

<div id="collapse-8-ICAPS-2019" class="collapse" role="tabpanel" data-parent="#accordion-ICAPS-2019">
<div class="card-body">
<p style="color:black;">
    A critical challenge in the design of AI systems that operate with humans in the loop is to be able to model the intentions and capabilities of the humans, as well as their beliefs and expectations of the AI system itself. This allows the AI system to be human-aware -- i.e. the human task model enables it to envisage desired roles of the human in joint action, while the human mental model allows it to anticipate how its own actions are perceived from the point of view of the human. In my research, I explore how these concepts of human-awareness manifest themselves in the scope of planning or sequential decision making with humans in the loop. To this end, I show: (1) How an AI agent can leverage the human task model to generate symbiotic behavior; and (2) How the introduction of the human mental model in the deliberative process of the AI agent allows it to generate explanations for a plan or resort to explicable plans when explanations are not desired.
The latter is in addition to traditional notions of human-aware planning which typically use the human task model alone and thus enables a new suite of capabilities of a human-aware AI agent. Finally, I explore how the AI agent can leverage emerging mixed-reality interfaces to realize effective channels of communication with the human in the loop.
</p>
</div>
<hr>
</div>

</div>
<div>

<div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-9-ICAPS-2019" aria-expanded="false" aria-controls="collapse-9-ICAPS-2019">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Explicability? Legibility? Predictability? Transparency? Privacy? Security? The Emerging Landscape of Interpretable Robot Behavior</strong>
</div>
<span class="d-block">Tathagata Chakraborti, Anagha Kulkarni, Sarath Sreedharan, David Smith and Subbarao Kambhampati</span>
<br>
<span class="d-block"><em>Jul 14 Sunday <b>2:40pm</b> in Sessions 8a: Recognition, Goal and Model Reasoning</em></span>
<a class="paper-link text-right badge badge-success" href="https://yochan-lab.github.io/papers/files/papers/landscape.pdf" target="_blank">Link</a>

</div>

</div>

<div id="collapse-9-ICAPS-2019" class="collapse" role="tabpanel" data-parent="#accordion-ICAPS-2019">
<div class="card-body">
<p style="color:black;">
    There has been significant interest of late in generating behavior of agents that is interpretable to the human (observer) in the loop. However, the work in this area has typically lacked coherence on the topic, with proposed solutions for "explicable", "legible", "predictable" and "transparent" planning with overlapping, and sometimes conflicting, semantics all aimed at some notion of understanding what intentions the observer will ascribe to an agent by observing its behavior. This is also true for the recent works on "security" and "privacy" of plans which are also trying to answer the same question, but from the opposite point of view -- i.e. when the agent is trying to
hide instead of reveal its intentions. This paper attempts to provide a workable taxonomy of relevant concepts in this exciting and emerging field of inquiry.
</p>
</div>
<hr>
</div>

</div>


</div>

</div>

</div>


        </main>

        <div class="footer-container">

            <div class="footer-bottom-left" style="width:100%;">

                <hr>

                <footer class="text-center" style="color:gray;">
                    <p>&copy; Yochan</p>
                </footer>

            </div>

        </div>

    </body>

</html>