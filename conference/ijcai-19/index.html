<!DOCTYPE html>

<html lang="en">

    <head>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

        <link rel="icon" href="../../favicon.ico">

        <title>Yochan @ IJCAI 2019</title>

        <!-- Bootstrap core CSS -->
        <link href="../../ASSETS/bootstrap-4.0.0/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom styles for this template -->
        <link href="./../files/css/custom.css" rel="stylesheet">

        <!-- jQuery -->
        <script src="../../ASSETS/jquery/jquery-3.2.1.min.js"></script>

        <!-- Bootstrap core JavaScript
        ================================================== -->
        <script src="../../ASSETS/bootstrap-4.0.0/js/popper.min.js"></script>
        <script src="../../ASSETS/bootstrap-4.0.0/js/bootstrap.min.js"></script>

        <!-- Custom JavaScript -->
        <script src="./../files/js/custom.js"></script>

    </head>

    <body class="bg-light" id="body" data-spy="scroll" data-target="#body" data-offset="100">

        <nav class="navbar navbar-expand-md fixed-top navbar-dark bg-dark">
            <li class="navbar-brand spy-enabled text-light"><strong>Yochan @ IJCAI 2019</strong></li>
        </nav>

        <div class="nav-scroller bg-white box-shadow">
            <nav class="nav nav-underline">
                <a class="nav-link bold" href="http://rakaposhi.eas.asu.edu/yochan/">Yochan</a>
                <a class="nav-link bold" href="https://robotics.asu.edu/">ASU Robotics</a>
                <a class="nav-link nav-link-no-link d-none d-sm-block">Last Updated <span class="badge badge-pill bg-light align-text-bottom">2019-08-07</span></a>
            </nav>
        </div>

        <main role="main" class="container">

        <div class="d-flex align-items-center p-3 my-3 text-white-50 bg-maroon rounded box-shadow">
            <img class="mr-3" src="./../files/images/icon.png" width="48" height="48" style="border-radius: 3pt;">
            <div class="lh-100">
                <h6 class="mb-0 text-white lh-100">Yochan Lab | Arizona State University</h6>
                <small>PI: Subbarao Kambhampati | <a class="nav-link-yellow text-white-50" href="mailto:rao@asu.edu">rao@asu.edu</a> | <a class="nav-link-yellow text-white-50" href="https://twitter.com/rao2z">@rao2z</a> | <a class="nav-link-yellow text-white-50" href="http://rakaposhi.eas.asu.edu/">rakaposhi.eas.asu.edu</a> </small>
            </div>
        </div>

<div class="my-3 p-3 bg-white rounded box-shadow" id="IJCAI-2019">

<h6 class="border-bottom border-gray pb-2 mb-0">IJCAI 2019</h6>

<div id="IJCAI-2019">

<div id="accordion-IJCAI-2019" role="tablist">

<div>

<div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-1-IJCAI-2019" aria-expanded="false" aria-controls="collapse-1-IJCAI-2019">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Balancing Explicability and Explanations for Human-Aware Planning</strong>
</div>
<span class="d-block">Tathagata Chakraborti, Sarath Sreedharan, Subbarao Kambhampati</span>
<br>
<span class="d-block"><em>Wednesday 14.  15:00 - 16:00. HAI | HAIC - Human-AI Collaboration</em></span>
<a class="paper-link text-right badge badge-warning" href="https://yochan-lab.github.io/papers/files/papers/balancing.pdf" target="_blank">Link</a>

</div>

</div>

<div id="collapse-1-IJCAI-2019" class="collapse" role="tabpanel" data-parent="#accordion-IJCAI-2019">
<div class="card-body">
<p style="color:black;">
    Human-aware planning involves generating plans that are explicable as well as providing explanations when such plans cannot be found. In this paper, we bring these two concepts together and show how an agent can achieve a trade-off between these two competing characteristics of a plan. In order to achieve this, we conceive a first of its kind planner mega that can augment the possibility of explaining a plan in the plan generation process itself. We situate our discussion in the context of recent work on explicable planning and explanation generation, and illustrate these concepts in two well-known planning domains, as well as in a demonstration of a robot in a typical search and reconnaissance task. Human factor studies in the latter highlight the usefulness of the proposed approach
</p>
</div>
<hr>
</div>

</div>
<div>

<div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-2-IJCAI-2019" aria-expanded="false" aria-controls="collapse-2-IJCAI-2019">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Why Can't You Do That HAL? Explaining Unsolvability of Planning Tasks</strong>
</div>
<span class="d-block">Sarath Sreedharan, Siddharth Srivastava, David Smith, Subbarao Kambhampati</span>
<br>
<span class="d-block"><em>Wednesday 14.  15:00 - 16:00. HAI | HAIC - Human-AI Collaboration</em></span>
<a class="paper-link text-right badge badge-warning" href="https://yochan-lab.github.io/papers/files/papers/IJCAI_19_Camera_Ready_Explaining_Unsolvability.pdf" target="_blank">Link</a>

</div>

</div>

<div id="collapse-2-IJCAI-2019" class="collapse" role="tabpanel" data-parent="#accordion-IJCAI-2019">
<div class="card-body">
<p style="color:black;">
    Explainable planning is widely accepted as a prerequisite for autonomous agents to successfully work with humans. While there has been a lot of research on generating explanations of solutions to planning problems, explaining the absence of solutions remains a largely open and under-studied problem, even though such situations can be the hardest to understand or debug. In this paper, we show that hierarchical abstractions can be used to efficiently generate reasons for unsolvability of planning problems. In contrast to related work on computing certificates of unsolvability, we show that our methods can generate compact, human-understandable reasons for unsolvability. Empirical analysis and user studies show the validity of our methods as well as their computational efficacy on a number of benchmark planning domains.
</p>
</div>
<hr>
</div>

</div>
<div>

<div class="media pt-3 entry-link" data-toggle="collapse" href="#collapse-3-IJCAI-2019" aria-expanded="false" aria-controls="collapse-3-IJCAI-2019">
<div class="media-body pb-3 mb-0 small lh-125 border-bottom border-gray">
<div class="d-flex justify-content-between align-items-center w-100">
<strong class="text-gray-dark">Modelfree Model Reconciliation</strong>
</div>
<span class="d-block">Sarath Sreedharan, Alberto Olmo, Aditya Prasad Mishra and Subbarao Kambhampati</span>
<br>
<span class="d-block"><em>Wednesday 14.  15:00 - 16:00. HAI | HAIC - Human-AI Collaboration</em></span>
<a class="paper-link text-right badge badge-warning" href="https://yochan-lab.github.io/papers/files/papers/ModelFree_Explanations_IJCAI19.pdf" target="_blank">Link</a>

</div>

</div>

<div id="collapse-3-IJCAI-2019" class="collapse" role="tabpanel" data-parent="#accordion-IJCAI-2019">
<div class="card-body">
<p style="color:black;">
    Designing agents capable of explaining complex sequential decisions remains a significant open problem in human-AI interaction. Recently, there has been a lot of interest in developing approaches for generating such explanations for various decision-making paradigms. One such approach has been the idea of explanation as model-reconciliation. The framework hypothesizes that one of the common reasons for a user's confusion could be the mismatch between the user's model of the agent's task model and the model used by the agent to generate the decisions. While this is a general framework, most works that have been explicitly built on this explanatory philosophy have focused on classical planning settings where the model of user's knowledge is available in a declarative form. Our goal in this paper is to adapt the model reconciliation approach to a more general planning paradigm and discuss how such methods could be used when user models are no longer explicitly available. Specifically, we present a simple and easy to learn labeling model that can help an explainer decide what information could help achieve model reconciliation between the user and the agent with in the context of planning with MDPs.
</p>
</div>
<hr>
</div>

</div>


</div>

</div>

</div>


        </main>

        <div class="footer-container">

            <div class="footer-bottom-left" style="width:100%;">

                <hr>

                <footer class="text-center" style="color:gray;">
                    <p>&copy; Yochan</p>
                </footer>

            </div>

        </div>

    </body>

</html>